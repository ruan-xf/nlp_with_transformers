
我对原notebook的这段表述很不理解
```
我们将其移位，使解码器只看到以前的地面真实标签，而不是现在或未来的标签。仅仅移位就足够了，因为解码器有掩蔽的自我注意力，可以掩蔽现在和未来的所有输入。

因此，当我们准备我们的批次时，我们通过将标签向右移动一个来设置解码器的输入。之后，我们确保标签中的填充标记被损失函数忽略，将它们设置为-100。不过，我们实际上不需要手动做这些，因为DataCollatorForSeq2Seq来拯救我们，为我们处理所有这些步骤:
```


于是想看看DataCollatorForSeq2Seq究竟输出了什么（见于代码）

其实这个效果就是了啊，讲那么多屁话
1. **自动移位标签**创建`decoder_input_ids`（移位后的标签）
2. **自动将填充标记替换为-100**，-100会被损失函数自动忽略，避免填充部分影响模型训练
3. **处理变长序列**的批处理：`input_ids.shape [3, 25]` 和 `labels.shape [3, 9]`，并且可以看到pad_token_id
4. **与模型的decoder架构配合**实现因果掩码


